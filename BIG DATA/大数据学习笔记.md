一、大数据计算服务考试内容

1.1 熟悉大数据计算服务基本概念，包括项目空间、表、分区、资源、任务等

	大数据计算服务是一种快速、完全托管的TB/PB级数据仓库解决方案。以及针对大数据的分析建模服务。
	
	数据通道(tunnel)：提供高并发的离线数据上传下载服务
	
	计算及分析任务：MaxCompute SQL不支持事务、索引及Update/Delete等操作
	
	MapReduce：最早是由Google提出的分布式数据处理模型、Java编程接口
	
	Graph：一套面向迭代的图计算处理框架
	
	项目空间：基本组织单元，类似于传统数据库的Database或Schema的概念，是进行多用户隔离和访问控制的主要边界 use my_project --进入项目空间
	
	表：数据存储单元，MaxCompute仅支持读取外部表数据，OSS，不支持写入
	
	分区：创建表时指定分区空间，分区列
	
	资源：使用函数或MapReduce功能需要依赖资源来完成，类型：file、table、jar、archive压缩
	
	数据类型：
		bigint	8字节有符号整型
		string	字符串，支持UTF-8编码 最长允许8MB
		boolean	布尔型
		double	8字节双精度浮点数
		datetime	日期类型
		decimal	有效数字，整数36位，小数18位
	
	任务：基本的计算单元
	
	服务连接：service.odps.aliyun.com 地址
			dt.odps.aliyun.com 地址

1.2 了解大数据计算服务的组成架构和各组成部分功能

	组成架构：4层
		客户端、接入层、逻辑层、计算层
	
	客户端：以RESTful API方式对外提供服务，用户可以通过SDK、CLT、Java集成开发环境、管理控制台、R语言集成开发环境
	
	接入层：LVS负载均衡  HTTP Server 云账号服务器
	
	逻辑层：接入层 请求处理器 调度器 作业执行管理器
		Worker Scheduler Excutor，三权分立，三者分别实现不同的逻辑
	
	计算层：架构在飞天系统之上，把大任务分解成很多小任务分别执行，最后把处理结果聚合，返回最后的结果
		分布式文件系统 盘古
		任务调度/资源管理	伏羲
		远程过程调用	夸父
		安全管理		钟馗
		协调服务		女娲

1.3 掌握大数据计算服务的特点、优势以及适用场景

	特点：提供安全、可靠、海量数据的快速采集、分析、建模、运用的一个数据仓库工具，学习门槛低，只要关心业务，无需关心分布式底层设计，同时只是针对结构化的数据进行处理

	优势：无需关心分布式计算、存储、基础架构方面的设计和落实，只要关心核心的业务设计，安全，可靠，易用，效率高，成本低
	
	适用场景：贷款的大数据应用
	

1.4 掌握大数据计算服务的连接和使用方式，包括使用客户端、管理控制台、Java SDK 等

	通过odpscmd连接：安装和配置JDK，把odpscmd_public里面的config.ini配置一下
	
	通过管理控制台连接：
		创建和管理项目
		创建和管理表
		查看和执行作业
		用户管理
		角色管理
		安全管理
		安全管理里面主要是几种授权模式的设置，例如：ACL授权、policy授权等
	
	通过Java SDK连接：
	
1.5 掌握大数据计算服务的数据上传和下载，可以熟练的使用Tunnel 命令行工具，了解 Tunnel SDK

	tunnel SDK：表数据的上传和下载
	TableTunnel:
			createDownloadSession创建下载会话的对象
			createUploadSession创建上传会话的对象
			getDownloadSession获取下载会话对象句柄
			getUploadSession获取上传会话对象句柄
	
	UploadSession：
			commit上传会话完成、提交
			getBlockList得到成功上传的Block的列表
			getSchema得到上传表的schema
			getID得到上传会话的Id
			newRecord创建Record类型的记录
			openRecordWriter打开记录写入器
			
	DownloadSession：
			getID得到下载会话的Id
			getRecordCount得到要下载的目标表的记录条数
			getSchema得到上传表的schema
			getStatus得到目前的下载状态
			openRecordReader打开记录读取器
			
	tunnel命令行工具的使用：
	tunnel upload 
		-dpr 丢弃错误的记录
		-dfp 定义时间数据格式
		-fd 定义列分隔符，默认是逗号
		-mbr 可以容忍的最大错误数据条数
		-rd 定义行的分隔符，默认是回车符
		-s 定义执行前是否进行扫描
		-te 多线程设置，默认是1个线程
		-tz 时区设置，默认是东八区
	Example：tunnel upload log.txt test_project.test_table/p1="b1",p2="b2";
	tunnel download
		-dfp 下载的时间格式
		-fd 定义下载的列的分隔符
		-ni 空值处理，默认“”
		-rd 定义下载行的分隔符，默认回车符
		-te 定义下载时候的线程数
	Example：tunnel download test_project.test_table/p1="b1",p2="b2" log.txt

	
1.6 掌握大数据计算服务的 SQL 命令，包括 DDL、DML 以及常见内置函数(30分)

	数据更新
		追加插入
		覆盖插入
		数据删除
		逻辑运算符
	多表关联
		内关联
		左右关联
		全关联
		笛卡尔积
	特色功能
		多态分区
		多路插入
		union all
		mapjoin
	其它
		分区扫描裁剪
		null值处理
		子查询
		视图
		
	重点看看 lifecycle修改
	逻辑运算符
	所有和NULL相关的地方要重点关注
	表关联非常关键
	正则表达式考了一题
	
	a like b :
		如果a或b为null,返回null
		% 匹配任意多个字符
		_ 匹配单个字符
		\ 转义字符
		'aaa' like 'a__' = true
		'aaa' like 'a%' = true
		'aaa' like 'aab' = false
		'a%b' like 'a\%b' = true
		'axb' like 'a\%b' = false
	
	a rlike b :
		a是字符串，b是字符串常量正则表达式
		如果b为空串会报错退出；
		如果a或b为null,返回null
		
	a in b :
		b是一个集合，如果a为null，返回null
		如果a在b中返回true，否则返回false
		如果b仅有一个元素null，则a in null返回null
		若b含有null元素，将null视为b集合中其他元素类型，b必须是常数并且至少有一项，所有类型要一致
	
	like与rlike的区别：
	like不是正则，而是通配符
	例如：select "aaaaa" rlike ".*aaa.*" from test_struct limit 10;结果是true
	逻辑操作符：
	false and null =false
	null and false = false
	优先级 true>null>false
	
	快速建表
		CTAS：两种谓语动词like和as
			create table t1 as select c1 from t2 where...;
			create table t1 like t2;
	
		as 和 like区别：
		数据：as可以带入数据，可以依赖于多张表
				like只能复制单张表结构，不能带入数据
		属性：as不能带入lifecycle、分区键信息、注释等
				like不能带入lifecycle，可以带入分区键信息、注释等
				
		简单查询：
			1.常见的简单查询：
			//行过滤和列选择
			select 8from t_dml where province='浙江省';
			select city,amt from t_dml where sale_date >='2019-01-07 00:00:00';
			select distinct city from t_dml where amt>700;
			2.使用子句的查询
			//group by +order by +limit
			select city,sum(amt) as total_amt from t_dml where provice='浙江省' group by city having count(*)>1 and sum(amt)>2000 order by total_amt desc limit 10;
			//distribute by +sort by
			select city,cnt,ant from t_dml distribute by city order by cnt;
			
		表连接(join)
		支持多路间接，但不支持笛卡尔积
			join_table:
				table_reference join table_factor[join_condition]
				table_reference {left outer|right outer|full outer|inter} join table_reference join_condition
			table_reference:
				table_factor
				join_table
			table_factor:
				tb1_name[alias]
				table_subquery alias
				(table_reference)
			join_condition:
				on equality_expression(and equality_expression)*
				
		MAPJOIN HINT
		1.能干普通join干不了的事
			select /*+mapjoin(a)*/ a.total_price,b.total_price from shop a join sale_detail b on a.total_price<b.total_price or a.total_price +b.total_price<500; 不等值条件下只能用mapjoin hint
		2.某些场合下，干的比普通join好
			select /*+mapjoin(a)*/ a.shop_name,b.customer_id,b.total_price from shop a join sale_detail b on a.shop_name=b.shop_name;查询速度快
			
		多路输出(multiinseret):
			支持在一个语句中插入不同的结果表或者分区
			form from_statement
			insert overwrite|into table tablename1
				select_statement1 [insert overwrite|into table tablename2 select_sattement2]
			多路输出的限制：
				·单个SQL里最多可以写128路输出
				·对于分区表，该表不能作为目标表出现多次
				·对于同一张分区表的不同分区，不能同时有insert overwrite 和insert into操作

	多路输出
		from sales
		insert into table t_dml_01
		select detail_id,sale_date,province,city,product_id,cnt,amt
		where detail_id>5340000
		insert overwrite table t_dml_p partition (sale_date='20190107')
		select detail_id,province,city,product_id,cnt,amt
		where sale_date>='2019-01-07 00:00:00'
		and sale_date<='2019-01-07 23:59:59'
		insert overwrite table t_dml_p partition (sale_date='20190108')
		select detail_id,province,city,product_id,cnt,amt
		where sale_date>='2019-01-08 00:00:00'
		and sale_date<='2019-01-08 23:59:59'
	简单case函数
	case sex
		when '1' then '男'
		when '2' then '女'
		else '其他' end
	--case搜索函数
	case when sex ='1' then '男'
		 when sex ='2' then '女'
		 else '其他' end
		 二者的区别就是在判断条件里面的判断变量是写在外面还是写在里面，如果写在外面，再写判断条件的时候，里面的变量名字就不用写了，直接写值就可以了

	动态分区
		在insert overwrite到一张分区表时，可以在语句中指定分区的值，也可以用另外一种更加灵活的方式，在分区中指定一个分区列名，但不给出值。相应的，在select子句中的对应列来提供分区的值
			动态分区功能的限制：
			·在分布式环境下，单个进程最多只能输出512个动态分区
			·任意动态分区SQL不可以生成超过2000个动态分区
			·动态生成的分区值不可以未null
			·如果目标表有多级分区，在运行insert语句时允许指定部分分区为静态，但是静态分区必须是高级分区
			alter table t_dml_p drop if exists partition(sale_date='20190107');
			alter table t_dml_p drop if exists partition(sale_date='20190108');
			
			insert into table t_dml_p partition(sale_date)
			select detail_id,province,city,product_id,cnt,amt,to_char(sale_date,'yyyymmdd')as sale_date from t_dml;

			insert overwrite table t_dml_p partition(sale_date)
			select detail_id,provice,city,product_id,cnt,amt,to_char(sale_date,'yyyymmdd')as sale_date from t_dml;

	内置函数介绍和使用：
		1.内置函数
			字符函数 匹配 截取 正则 连接等
			数值函数 round/trunc/floor/...
			日期函数 计算日期差、得到日期...
			聚合函数 sum/std/avg/...
		2.自定义函数
			UDF 用户自定义标量函数
			UDAF 用户自定义聚组函数
			UDTF 用户自定义表值函数

		数学运算函数：
			三角类
				ACOS/COS
				ASIN/SIN
				ATAN
				COSH
				COT
				SINH
				TAN
				TANH
			整形类
				CEIL
				FLOOR
				ROUND
				TRUNC
				CONV
			运算类
				ABS
				EXP
				LN
				LOG
				POW
				SQRT
			随机数
				RAND
		字符串处理类
			长度类
				LENGTH
				LENGTHB
			查找类
				CHAR_MATCHCOUNT
				INSTR
				SUBSTR
			转换类
				CHR
				IS_ENCODING
				MD5
				TO_CHAR
			整形类
				CONCAT
				SPLIT_PART
				TOLOWER
				TOUPPER
				TRIM
			正则类
				REGEXP_EXTRACT
				REGEXP_INSTR
				REGEXP_REPLACE
				REGEXP_SUBSTR
				REGEXP_COUNT
		日期类型处理函数
			日期获取类
				GETDATE
				LASTDAY
				DATEPART
				DATETRUNC
				WEEKDAY
				WEEKOFYEAR
			日期转换类
				FROM_UNIXTIME
				TO_DATE
				TO_CHAR
				UNIX_TIMESTAMP
				ISDATE
			日期计算类
				DATEADD
				DATEDIFF

		窗口函数
			统计量
				COUNT
				SUM
				AVG
				MAX/MIN
				MEDIAN
				SEDDEV
				SEDDEV_SAMP
			排名类
				ROW_NUMBER
				RANK
				DENSE_RNAK
				PERCENT_RANK
			其他类
				LAG
				LEAD
				CLUSTER_SAMPLE
		窗口函数的用法
			基本语法：
			window_func() over(partition by coll,[col2...])
			把数据按照一定条件分成多组称为开窗，每个组称为一个窗口
			partition by部分用来指定开窗的列
			分区列的值相同的行被视为在同一个窗口内
			order by用来指定数据在一个窗口内融合排序
		窗口函数的限制
			使用限制
				·只能出现在select子句中
				·窗口函数中不要乔涛使用窗口函数和聚合函数
				·不可以和同级别的聚合函数一起使用
				·一个SQL语句中，可以使用至多5个窗口函数
				·partition开窗时，同一个窗口内最多包含1亿行数据
				·用rows开窗时，x、y必须为大于等于0的整数常量，限定范围0~10000，值为0时表示当前行
				·必须指定order by才可以用rows方式指定窗口范围
				·并非所有的窗口函数都可以用rows指定开窗方式，支持这种用法的窗口函数有avg、count、max、min、stddev和sum

		开窗实例
			用户信息表中，存在名字重复的记录，想把每个名字对应的最大编号取出来，用窗口函数实现：
			select max(id) over(partition by name order by id desc),name,id from t_test;
		带rows的开窗
			时序分析中常常需要按照时间窗口对数据进行平滑处理，以消除噪音。尝试使用SQL的窗口函数，对一份销售数据进行平滑处理，以n天为单位
		平滑数据
			select pid,day,cnt,avg(cnt)over(partition by pid order by days rows between 3 preceding and 2 following) from t time windows;

		聚合函数
			count
			median
			avg
			stddev
			max/min
			stddev_samp
			sum
			wm_concat
		其他函数
			cast 类型转换函数
			coalesce 返回第一个非null的值
			decode 分支选择，实现if-then-else的功能
			greatest 返回最大值
			ordinal 返回排序后指定位置的值
			least 返回最小值
			uuid 返回随机id:8-4-4-16格式
			sample 采样，返回hash分布后的某份数据
				

1.7 熟悉大数据计算服务的用户自定义函数，包括 UDF、UDAF以及 UDTF，可以编写简单的自定义函数

	自定义函数：
		UDF全称User Definned Function即用户自定义函数，使用Maven的用户可以从Maven库中搜索“odps-sdk-udf”获取不同版本的Java SDK,相关配置信息：
		<dependency>
			<groupId>com.aliyun.odps</groupId>
			<artifactId>odps-sdk-udf</artifactId>
			<version>0.20.7-public</version>
		</dependency>
	自定义表值函数：
		UDTF全称User Defined Table Valued Function，是用来解决一次函数调用输出多汗数据场景的，意识唯一能返回多个字段的自定义函数，而UDF只能一次计算输出一条返回值
	自定义聚合函数：
		UDAF全称User Defined Aggregation Function其输入与输出是多对一的关系，即将多条输入记录聚合成一条输出值，可以与SQL中的Group By语句联用
	UDF支持SQL的数据类型有：bigint,string,double,boolean以及datetime类型，SQL中的null值通过java的null引用表示，因此Java primitive type是不允许使用的，因为无法表示SQL中的null值
	
	备注：
	UDF目前只支持Java语言接口，用户如果想编写UDF程序，可以通过添加资源的方式将UDF代码上传到项目空间中，使用注册函数语句创建UDF
	如果用户需要使用UDF功能，需要在工单系统上提交申请，提供odps project名称，申请通过，开通好权限后才可以创建UDF
	
	编写并使用UDF
		·编写Java代码，实现加密逻辑
		·本地测试
		·导出jar包，作为资源上传至ODPS
		·基于jar包创建函数
		·测试、使用该函数
	
	UDF示例：
		下面给出一个简单的代码实现：
		package org.alidata.odps.udf.example
		import com.aliyun.odps.udf.UDF;

		public final class Lower extends UDF{
			public String evaluate(String s){
				if(s==null){return null;}
				return s.toLowerCase();
			}
		}
		将这个jar包命名为“my_lower.jar”

		添加资源：在运行udf之前，必须指定的udf代码，用户代码通过资源的形式添加到odps中，java udf必须被打成jar包，以jar资源添加到odps中，udf框架会自动加载jar包，运行用户自定义的udf。MapReduce也用到了资源这一特有概念，执行命令；
		add jar my_lower.jar;
			--如果存在同名的资源请将这个jar包重命名
			--并注意修改下面示例命令中相关jar包的名字；
			--又或者直接使用-f选项覆盖原有的jar资源

		注册udf函数：用户的jar包被上传后，使得MaxCompute有条件自动获取用户代码并允许。但此时仍然无法使用这个udf，因为MaxCompute中并没有关于这个udf的任何信息。因此需要用户在MaxCompute中注册一个唯一的函数名，并指定这个函数名与哪个jar资源的哪个函数对应，运行命令：
		create function test_lower as org.alidata.odps.udf.examples.Lower using my_lower.jar;

		在sql中使用此函数：
		select test_lower('A')from my_test_table;

		函数操作
			语法：
				create function f1 as p1 using r1;
			说明：
			·funcation_name：udf函数名，这个名字就是SQL中引用该函数所使用的名字
			·package_to_class：如果是java UDF,这个名字就是从顶层包名一直到实现UDF类名的fully qualified class name
			·resource_list：udf所用到的资源列表，这个里面必须包括udf代码所在的资源，如果用户代码中通过distributed cache接口读取资源文件夹，这个列表中还得包括udf所读取的资源文件列表，资源列表必须用引号引起来
			·使用示例：假设java udf类org.alidata.odps.udf.examples.Lower在my_lower.jar中，创建函数my_lower:
				create function test_lower as 'org.alidata.odps.udf.excamples.Lower' using 'my_lower.jar';
				注解：同名函数只能注册一次
					一般情况下用户自建函数无法覆盖系统内建函数，只有项目空间的Owner才有权利覆盖内建函数，如果用户使用了覆盖内建函数的自定义函数，在SQL执行结束后，会在Summary中打印出warning信息
					
1.8 熟悉大数据计算服务的 MapReduce 编程框架，可以配置Eclipse 的环境，编写简单的 MR 程序

	MR的关键步骤：
		映射(Mapping)对集合里的每个目标应用同一个操作
		化简(Reducing)遍历集合，返回结果
	01基本概念
		涉及理念
		MR2
		键值对
		安全沙箱
	02应用场景
		分布式计算
		实时性不强
		任务不可拆分
	03开发测试
		环境配置搭建
		java开发步骤
		本地数据测试
	04处理流程
		分片split
		主要流程
		必须流程
		可选流程
	05MR调用
		调用命令
		命令参数
		输入输出
		使用资源
	06MR优化
		长尾效应
		性能提升

	MapReduce最早是有Google提出的分布式数据处理模型，随后受到了业内的广泛关注，并被大量应用到各种商业场景中，比如：
	·搜索：网页爬取、倒排索引、PageRank
	·web访问日志分析：分析和挖掘用户在web上的访问、购物行为特征，实现个性化推荐；分析用户访问行为
	·文本统计分析：比如莫言小说的wordcount、词频tfidf分析；学术论文、专利文献的引用分析和统计；维基百科数据分析等
	·海量数据挖掘：非结构化数据、时空数据、图像数据的挖掘
	·机器学习：监督学习、无监督学习、分类算法如决策树、SVM等
	·自然语言处理：基于大数据的训练和预测；基于语料库构建单词同现矩阵，频繁项集数据挖掘、重复文档检测等
	广告推荐：用户点击CTR和购买行为CVR预测

	MapReduce处理数据过程主要分为2个阶段：Map阶段和Reduce阶段。首先执行Map阶段，再试下Reduce阶段。Map和Reduce的处理逻辑由用户自定义实现，但要符合MapReduce框架的约定

	MR执行流程：
	1.提交任务
	2.轮询任务状态
	3.获取任务结束信息
		Map阶段：
		setup();
		for each record{
			map();统计worker内的counter
		}
		cleanup();
		结束时向框架汇报worker内部counter的信息

		Reduce阶段：
		setup();
		for each key{
			reduce();统计worker内的counter
		}
		cleanup();
		结束时向框架汇报worker内部counter的信息

		计算任务结束后：
			统计所有worker的counter信息
			查询任务信息并做记录
			查询任务执行的Summary信息做记录

		·在正式执行Map前，需要将输入数据进行“分片”，所谓分片，就是将输入数据切分未大小相等的数据块，每一块作为单个Map Worker的输入被处理，以便于多个Map Worker同时工作
		·分片完毕后，多个Map Worker就可以同时工作了，每个Map Worker计算处理，最终输出给Reduce，Map Worker在输出数据时，需要为每一条输出数据指定一个key,这个key值决定了这条数据将会被发送给哪一个Reduce worker。key值和reduce worker是多对一的关系，单个Reduce Worker有可能会接收到多个key值的数据
		·使得具有相同key的数据彼此相邻，如果用户指定了“合并操作”(combiner)，框架会调用combiner，将相同的key的数据进行聚合。这部分的处理通常也叫做“洗牌”(shuffle)

		备注：MaxCompute MapReduce的输入、输出只能是表，不允许用户自定义输出格式，不提供类似文件系统的接口

