一、大数据计算服务考试内容

1.1 熟悉大数据计算服务基本概念，包括项目空间、表、分区、资源、任务等

	大数据计算服务是一种快速、完全托管的TB/PB级数据仓库解决方案。以及针对大数据的分析建模服务。
	
	数据通道(tunnel)：提供高并发的离线数据上传下载服务
	
	计算及分析任务：MaxCompute SQL不支持事务、索引及Update/Delete等操作
	
	MapReduce：最早是由Google提出的分布式数据处理模型、Java编程接口
	
	Graph：一套面向迭代的图计算处理框架
	
	项目空间：基本组织单元，类似于传统数据库的Database或Schema的概念，是进行多用户隔离和访问控制的主要边界 use my_project --进入项目空间
	
	表：数据存储单元，MaxCompute仅支持读取外部表数据，OSS，不支持写入
	
	分区：创建表时指定分区空间，分区列
	
	资源：使用函数或MapReduce功能需要依赖资源来完成，类型：file、table、jar、archive压缩
	
	数据类型：
		bigint	8字节有符号整型
		string	字符串，支持UTF-8编码 最长允许8MB
		boolean	布尔型
		double	8字节双精度浮点数
		datetime	日期类型
		decimal	有效数字，整数36位，小数18位
	
	任务：基本的计算单元
	
	服务连接：service.odps.aliyun.com 地址
			dt.odps.aliyun.com 地址

1.2 了解大数据计算服务的组成架构和各组成部分功能

	组成架构：4层
		客户端、接入层、逻辑层、计算层
	
	客户端：以RESTful API方式对外提供服务，用户可以通过SDK、CLT、Java集成开发环境、管理控制台、R语言集成开发环境
	
	接入层：LVS负载均衡  HTTP Server 云账号服务器
	
	逻辑层：接入层 请求处理器 调度器 作业执行管理器
		Worker Scheduler Excutor，三权分立，三者分别实现不同的逻辑
	
	计算层：架构在飞天系统之上，把大任务分解成很多小任务分别执行，最后把处理结果聚合，返回最后的结果
		分布式文件系统 盘古
		任务调度/资源管理	伏羲
		远程过程调用	夸父
		安全管理		钟馗
		协调服务		女娲

1.3 掌握大数据计算服务的特点、优势以及适用场景

	特点：提供安全、可靠、海量数据的快速采集、分析、建模、运用的一个数据仓库工具，学习门槛低，只要关心业务，无需关心分布式底层设计，同时只是针对结构化的数据进行处理

	优势：无需关心分布式计算、存储、基础架构方面的设计和落实，只要关心核心的业务设计，安全，可靠，易用，效率高，成本低
	
	适用场景：贷款的大数据应用
	

1.4 掌握大数据计算服务的连接和使用方式，包括使用客户端、管理控制台、Java SDK 等

	通过odpscmd连接：安装和配置JDK，把odpscmd_public里面的config.ini配置一下
	
	通过管理控制台连接：
		创建和管理项目
		创建和管理表
		查看和执行作业
		用户管理
		角色管理
		安全管理
		安全管理里面主要是几种授权模式的设置，例如：ACL授权、policy授权等
	
	通过Java SDK连接：
	
1.5 掌握大数据计算服务的数据上传和下载，可以熟练的使用Tunnel 命令行工具，了解 Tunnel SDK

	tunnel SDK：表数据的上传和下载
	TableTunnel:
			createDownloadSession创建下载会话的对象
			createUploadSession创建上传会话的对象
			getDownloadSession获取下载会话对象句柄
			getUploadSession获取上传会话对象句柄
	
	UploadSession：
			commit上传会话完成、提交
			getBlockList得到成功上传的Block的列表
			getSchema得到上传表的schema
			getID得到上传会话的Id
			newRecord创建Record类型的记录
			openRecordWriter打开记录写入器
			
	DownloadSession：
			getID得到下载会话的Id
			getRecordCount得到要下载的目标表的记录条数
			getSchema得到上传表的schema
			getStatus得到目前的下载状态
			openRecordReader打开记录读取器
			
	tunnel命令行工具的使用：
	tunnel upload 
		-dpr 丢弃错误的记录
		-dfp 定义时间数据格式
		-fd 定义列分隔符，默认是逗号
		-mbr 可以容忍的最大错误数据条数
		-rd 定义行的分隔符，默认是回车符
		-s 定义执行前是否进行扫描
		-te 多线程设置，默认是1个线程
		-tz 时区设置，默认是东八区
	Example：tunnel upload log.txt test_project.test_table/p1="b1",p2="b2";
	tunnel download
		-dfp 下载的时间格式
		-fd 定义下载的列的分隔符
		-ni 空值处理，默认“”
		-rd 定义下载行的分隔符，默认回车符
		-te 定义下载时候的线程数
	Example：tunnel download test_project.test_table/p1="b1",p2="b2" log.txt

	
1.6 掌握大数据计算服务的 SQL 命令，包括 DDL、DML 以及常见内置函数(30分)

	数据更新
		追加插入
		覆盖插入
		数据删除
		逻辑运算符
	多表关联
		内关联
		左右关联
		全关联
		笛卡尔积
	特色功能
		多态分区
		多路插入
		union all
		mapjoin
	其它
		分区扫描裁剪
		null值处理
		子查询
		视图
		
	重点看看 lifecycle修改
	逻辑运算符
	所有和NULL相关的地方要重点关注
	表关联非常关键
	正则表达式考了一题
	
	a like b :
		如果a或b为null,返回null
		% 匹配任意多个字符
		_ 匹配单个字符
		\ 转义字符
		'aaa' like 'a__' = true
		'aaa' like 'a%' = true
		'aaa' like 'aab' = false
		'a%b' like 'a\%b' = true
		'axb' like 'a\%b' = false
	
	a rlike b :
		a是字符串，b是字符串常量正则表达式
		如果b为空串会报错退出；
		如果a或b为null,返回null
		
	a in b :
		b是一个集合，如果a为null，返回null
		如果a在b中返回true，否则返回false
		如果b仅有一个元素null，则a in null返回null
		若b含有null元素，将null视为b集合中其他元素类型，b必须是常数并且至少有一项，所有类型要一致
	
	like与rlike的区别：
	like不是正则，而是通配符
	例如：select "aaaaa" rlike ".*aaa.*" from test_struct limit 10;结果是true
	逻辑操作符：
	false and null =false
	null and false = false
	优先级 true>null>false
	
	快速建表
		CTAS：两种谓语动词like和as
			create table t1 as select c1 from t2 where...;
			create table t1 like t2;
	
		as 和 like区别：
		数据：as可以带入数据，可以依赖于多张表
				like只能复制单张表结构，不能带入数据
		属性：as不能带入lifecycle、分区键信息、注释等
				like不能带入lifecycle，可以带入分区键信息、注释等
				
		简单查询：
			1.常见的简单查询：
			//行过滤和列选择
			select 8from t_dml where province='浙江省';
			select city,amt from t_dml where sale_date >='2019-01-07 00:00:00';
			select distinct city from t_dml where amt>700;
			2.使用子句的查询
			//group by +order by +limit
			select city,sum(amt) as total_amt from t_dml where provice='浙江省' group by city having count(*)>1 and sum(amt)>2000 order by total_amt desc limit 10;
			//distribute by +sort by
			select city,cnt,ant from t_dml distribute by city order by cnt;
			
		表连接(join)
		支持多路间接，但不支持笛卡尔积
			join_table:
				table_reference join table_factor[join_condition]
				table_reference {left outer|right outer|full outer|inter} join table_reference join_condition
			table_reference:
				table_factor
				join_table
			table_factor:
				tb1_name[alias]
				table_subquery alias
				(table_reference)
			join_condition:
				on equality_expression(and equality_expression)*
				
		MAPJOIN HINT
		1.能干普通join干不了的事
			select /*+mapjoin(a)*/ a.total_price,b.total_price from shop a join sale_detail b on a.total_price<b.total_price or a.total_price +b.total_price<500; 不等值条件下只能用mapjoin hint
		2.某些场合下，干的比普通join好
			select /*+mapjoin(a)*/ a.shop_name,b.customer_id,b.total_price from shop a join sale_detail b on a.shop_name=b.shop_name;查询速度快
			
		多路输出(multiinseret):
			支持在一个语句中插入不同的结果表或者分区
			form from_statement
			insert overwrite|into table tablename1
				select_statement1 [insert overwrite|into table tablename2 select_sattement2]
			多路输出的限制：
				·单个SQL里最多可以写128路输出
				·对于分区表，该表不能作为目标表出现多次
				·对于同一张分区表的不同分区，不能同时有insert overwrite 和insert into操作

	多路输出
		from sales
		insert into table t_dml_01
		select detail_id,sale_date,province,city,product_id,cnt,amt
		where detail_id>5340000
		insert overwrite table t_dml_p partition (sale_date='20190107')
		select detail_id,province,city,product_id,cnt,amt
		where sale_date>='2019-01-07 00:00:00'
		and sale_date<='2019-01-07 23:59:59'
		insert overwrite table t_dml_p partition (sale_date='20190108')
		select detail_id,province,city,product_id,cnt,amt
		where sale_date>='2019-01-08 00:00:00'
		and sale_date<='2019-01-08 23:59:59'
	简单case函数
	case sex
		when '1' then '男'
		when '2' then '女'
		else '其他' end
	--case搜索函数
	case when sex ='1' then '男'
		 when sex ='2' then '女'
		 else '其他' end
		 二者的区别就是在判断条件里面的判断变量是写在外面还是写在里面，如果写在外面，再写判断条件的时候，里面的变量名字就不用写了，直接写值就可以了

	动态分区
		在insert overwrite到一张分区表时，可以在语句中指定分区的值，也可以用另外一种更加灵活的方式，在分区中指定一个分区列名，但不给出值。相应的，在select子句中的对应列来提供分区的值
			动态分区功能的限制：
			·在分布式环境下，单个进程最多只能输出512个动态分区
			·任意动态分区SQL不可以生成超过2000个动态分区
			·动态生成的分区值不可以未null
			·如果目标表有多级分区，在运行insert语句时允许指定部分分区为静态，但是静态分区必须是高级分区
			alter table t_dml_p drop if exists partition(sale_date='20190107');
			alter table t_dml_p drop if exists partition(sale_date='20190108');
			
			insert into table t_dml_p partition(sale_date)
			select detail_id,province,city,product_id,cnt,amt,to_char(sale_date,'yyyymmdd')as sale_date from t_dml;

			insert overwrite table t_dml_p partition(sale_date)
			select detail_id,provice,city,product_id,cnt,amt,to_char(sale_date,'yyyymmdd')as sale_date from t_dml;

	内置函数介绍和使用：
		1.内置函数
			字符函数 匹配 截取 正则 连接等
			数值函数 round/trunc/floor/...
			日期函数 计算日期差、得到日期...
			聚合函数 sum/std/avg/...
		2.自定义函数
			UDF 用户自定义标量函数
			UDAF 用户自定义聚组函数
			UDTF 用户自定义表值函数

		数学运算函数：
			三角类
				ACOS/COS
				ASIN/SIN
				ATAN
				COSH
				COT
				SINH
				TAN
				TANH
			整形类
				CEIL
				FLOOR
				ROUND
				TRUNC
				CONV
			运算类
				ABS
				EXP
				LN
				LOG
				POW
				SQRT
			随机数
				RAND
		字符串处理类
			长度类
				LENGTH
				LENGTHB
			查找类
				CHAR_MATCHCOUNT
				INSTR
				SUBSTR
			转换类
				CHR
				IS_ENCODING
				MD5
				TO_CHAR
			整形类
				CONCAT
				SPLIT_PART
				TOLOWER
				TOUPPER
				TRIM
			正则类
				REGEXP_EXTRACT
				REGEXP_INSTR
				REGEXP_REPLACE
				REGEXP_SUBSTR
				REGEXP_COUNT
		日期类型处理函数
			日期获取类
				GETDATE
				LASTDAY
				DATEPART
				DATETRUNC
				WEEKDAY
				WEEKOFYEAR
			日期转换类
				FROM_UNIXTIME
				TO_DATE
				TO_CHAR
				UNIX_TIMESTAMP
				ISDATE
			日期计算类
				DATEADD
				DATEDIFF

		窗口函数
			统计量
				COUNT
				SUM
				AVG
				MAX/MIN
				MEDIAN
				SEDDEV
				SEDDEV_SAMP
			排名类
				ROW_NUMBER
				RANK
				DENSE_RNAK
				PERCENT_RANK
			其他类
				LAG
				LEAD
				CLUSTER_SAMPLE
		窗口函数的用法
			基本语法：
			window_func() over(partition by coll,[col2...])
			把数据按照一定条件分成多组称为开窗，每个组称为一个窗口
			partition by部分用来指定开窗的列
			分区列的值相同的行被视为在同一个窗口内
			order by用来指定数据在一个窗口内融合排序
		窗口函数的限制
			使用限制
				·只能出现在select子句中
				·窗口函数中不要乔涛使用窗口函数和聚合函数
				·不可以和同级别的聚合函数一起使用
				·一个SQL语句中，可以使用至多5个窗口函数
				·partition开窗时，同一个窗口内最多包含1亿行数据
				·用rows开窗时，x、y必须为大于等于0的整数常量，限定范围0~10000，值为0时表示当前行
				·必须指定order by才可以用rows方式指定窗口范围
				·并非所有的窗口函数都可以用rows指定开窗方式，支持这种用法的窗口函数有avg、count、max、min、stddev和sum

		开窗实例
			用户信息表中，存在名字重复的记录，想把每个名字对应的最大编号取出来，用窗口函数实现：
			select max(id) over(partition by name order by id desc),name,id from t_test;
		带rows的开窗
			时序分析中常常需要按照时间窗口对数据进行平滑处理，以消除噪音。尝试使用SQL的窗口函数，对一份销售数据进行平滑处理，以n天为单位
		平滑数据
			select pid,day,cnt,avg(cnt)over(partition by pid order by days rows between 3 preceding and 2 following) from t time windows;

		聚合函数
			count
			median
			avg
			stddev
			max/min
			stddev_samp
			sum
			wm_concat
		其他函数
			cast 类型转换函数
			coalesce 返回第一个非null的值
			decode 分支选择，实现if-then-else的功能
			greatest 返回最大值
			ordinal 返回排序后指定位置的值
			least 返回最小值
			uuid 返回随机id:8-4-4-16格式
			sample 采样，返回hash分布后的某份数据
				

1.7 熟悉大数据计算服务的用户自定义函数，包括 UDF、UDAF以及 UDTF，可以编写简单的自定义函数

	自定义函数：
		UDF全称User Definned Function即用户自定义函数，使用Maven的用户可以从Maven库中搜索“odps-sdk-udf”获取不同版本的Java SDK,相关配置信息：
		<dependency>
			<groupId>com.aliyun.odps</groupId>
			<artifactId>odps-sdk-udf</artifactId>
			<version>0.20.7-public</version>
		</dependency>
	自定义表值函数：
		UDTF全称User Defined Table Valued Function，是用来解决一次函数调用输出多汗数据场景的，意识唯一能返回多个字段的自定义函数，而UDF只能一次计算输出一条返回值
	自定义聚合函数：
		UDAF全称User Defined Aggregation Function其输入与输出是多对一的关系，即将多条输入记录聚合成一条输出值，可以与SQL中的Group By语句联用
	UDF支持SQL的数据类型有：bigint,string,double,boolean以及datetime类型，SQL中的null值通过java的null引用表示，因此Java primitive type是不允许使用的，因为无法表示SQL中的null值
	
	备注：
	UDF目前只支持Java语言接口，用户如果想编写UDF程序，可以通过添加资源的方式将UDF代码上传到项目空间中，使用注册函数语句创建UDF
	如果用户需要使用UDF功能，需要在工单系统上提交申请，提供odps project名称，申请通过，开通好权限后才可以创建UDF
	
	编写并使用UDF
		·编写Java代码，实现加密逻辑
		·本地测试
		·导出jar包，作为资源上传至ODPS
		·基于jar包创建函数
		·测试、使用该函数
	
	UDF示例：
		下面给出一个简单的代码实现：
		package org.alidata.odps.udf.example
		import com.aliyun.odps.udf.UDF;

		public final class Lower extends UDF{
			public String evaluate(String s){
				if(s==null){return null;}
				return s.toLowerCase();
			}
		}
		将这个jar包命名为“my_lower.jar”

		添加资源：在运行udf之前，必须指定的udf代码，用户代码通过资源的形式添加到odps中，java udf必须被打成jar包，以jar资源添加到odps中，udf框架会自动加载jar包，运行用户自定义的udf。MapReduce也用到了资源这一特有概念，执行命令；
		add jar my_lower.jar;
			--如果存在同名的资源请将这个jar包重命名
			--并注意修改下面示例命令中相关jar包的名字；
			--又或者直接使用-f选项覆盖原有的jar资源

		注册udf函数：用户的jar包被上传后，使得MaxCompute有条件自动获取用户代码并允许。但此时仍然无法使用这个udf，因为MaxCompute中并没有关于这个udf的任何信息。因此需要用户在MaxCompute中注册一个唯一的函数名，并指定这个函数名与哪个jar资源的哪个函数对应，运行命令：
		create function test_lower as org.alidata.odps.udf.examples.Lower using my_lower.jar;

		在sql中使用此函数：
		select test_lower('A')from my_test_table;

		函数操作
			语法：
				create function f1 as p1 using r1;
			说明：
			·funcation_name：udf函数名，这个名字就是SQL中引用该函数所使用的名字
			·package_to_class：如果是java UDF,这个名字就是从顶层包名一直到实现UDF类名的fully qualified class name
			·resource_list：udf所用到的资源列表，这个里面必须包括udf代码所在的资源，如果用户代码中通过distributed cache接口读取资源文件夹，这个列表中还得包括udf所读取的资源文件列表，资源列表必须用引号引起来
			·使用示例：假设java udf类org.alidata.odps.udf.examples.Lower在my_lower.jar中，创建函数my_lower:
				create function test_lower as 'org.alidata.odps.udf.excamples.Lower' using 'my_lower.jar';
				注解：同名函数只能注册一次
					一般情况下用户自建函数无法覆盖系统内建函数，只有项目空间的Owner才有权利覆盖内建函数，如果用户使用了覆盖内建函数的自定义函数，在SQL执行结束后，会在Summary中打印出warning信息
					
1.8 熟悉大数据计算服务的 MapReduce 编程框架，可以配置Eclipse 的环境，编写简单的 MR 程序

	MR的关键步骤：
		映射(Mapping)对集合里的每个目标应用同一个操作
		化简(Reducing)遍历集合，返回结果
	01基本概念
		涉及理念
		MR2
		键值对
		安全沙箱
	02应用场景
		分布式计算
		实时性不强
		任务不可拆分
	03开发测试
		环境配置搭建
		java开发步骤
		本地数据测试
	04处理流程
		分片split
		主要流程
		必须流程
		可选流程
	05MR调用
		调用命令
		命令参数
		输入输出
		使用资源
	06MR优化
		长尾效应
		性能提升

	MapReduce最早是有Google提出的分布式数据处理模型，随后受到了业内的广泛关注，并被大量应用到各种商业场景中，比如：
	·搜索：网页爬取、倒排索引、PageRank
	·web访问日志分析：分析和挖掘用户在web上的访问、购物行为特征，实现个性化推荐；分析用户访问行为
	·文本统计分析：比如莫言小说的wordcount、词频tfidf分析；学术论文、专利文献的引用分析和统计；维基百科数据分析等
	·海量数据挖掘：非结构化数据、时空数据、图像数据的挖掘
	·机器学习：监督学习、无监督学习、分类算法如决策树、SVM等
	·自然语言处理：基于大数据的训练和预测；基于语料库构建单词同现矩阵，频繁项集数据挖掘、重复文档检测等
	广告推荐：用户点击CTR和购买行为CVR预测

	MapReduce处理数据过程主要分为2个阶段：Map阶段和Reduce阶段。首先执行Map阶段，再试下Reduce阶段。Map和Reduce的处理逻辑由用户自定义实现，但要符合MapReduce框架的约定

	MR执行流程：
	1.提交任务
	2.轮询任务状态
	3.获取任务结束信息
		Map阶段：
		setup();
		for each record{
			map();统计worker内的counter
		}
		cleanup();
		结束时向框架汇报worker内部counter的信息

		Reduce阶段：
		setup();
		for each key{
			reduce();统计worker内的counter
		}
		cleanup();
		结束时向框架汇报worker内部counter的信息

		计算任务结束后：
			统计所有worker的counter信息
			查询任务信息并做记录
			查询任务执行的Summary信息做记录

		·在正式执行Map前，需要将输入数据进行“分片”，所谓分片，就是将输入数据切分未大小相等的数据块，每一块作为单个Map Worker的输入被处理，以便于多个Map Worker同时工作
		·分片完毕后，多个Map Worker就可以同时工作了，每个Map Worker计算处理，最终输出给Reduce，Map Worker在输出数据时，需要为每一条输出数据指定一个key,这个key值决定了这条数据将会被发送给哪一个Reduce worker。key值和reduce worker是多对一的关系，单个Reduce Worker有可能会接收到多个key值的数据
		·使得具有相同key的数据彼此相邻，如果用户指定了“合并操作”(combiner)，框架会调用combiner，将相同的key的数据进行聚合。这部分的处理通常也叫做“洗牌”(shuffle)

		备注：MaxCompute MapReduce的输入、输出只能是表，不允许用户自定义输出格式，不提供类似文件系统的接口


1.9 了解大数据计算服务的 Graph 编程框架，包括基本概念、处理流程等，可以编写简单的 Graph 程序

	概念
		图的定义
			graph是一套面向迭代的图计算框架
		图的组成
			图计算作业使用图进行建模，图有点(vertex)和边(edge)组成，点和边包含权值(value)
		数据结构
		图计算实现
	场景
		单源最短距离
		K均值聚类
		PageRank
		社交网络分析
	流程
		迭代框架
		图加载
		节点分配
		超步与终止
	调用
		打包上传
		调用命令
		输入输出
		使用资源

	图加载：框架调用用户自定义的GraphLoader将输入表的记录解析为点或边；分布式化：框架调用用户自定义的Partitioner对点进行分片(默认分片逻辑：点ID哈希值然后对worker数取模)，分配到相应的worker

	迭代计算：
		·一次迭代为一个“超步”(superstep),遍历所有非结束状态(halted值为false)的点或者收到消息的点(处于结束状态的点收到信息会被自动唤醒)，并调用其compute方法
		·在用户实现compute方法中
		·处理上一个超步发给当前点的消息(Messages);
		·根据需要对图进行编辑：
			1.修改点/边的取值
			2.发送消息给某些点
			3.增加/删除点或边*
		·通过aggregator汇总信息导全局信息
		·设置当前点状态，结束或非结束状态
		·迭代进行过程中，框架会将消息以异步的方式发送到对应的worker并在喜爱一个超步进行处理，用户无需关心

	迭代终止：
		所有点处于结束状态(halted值为true)且没有新消息产生
		达到最大迭代次数
		某个aggregator的terminate方法返回true
		伪代码描述如下：
		//1.load
		for each record in input_table{
			GraphLoader.load();
		}
		//2.setup
		WorkerComputer.setup();
		for each aggr in aggregators{
			aggr.createStartupValue();
		}
		for each v in vertices{
			v.setup();
		}
		//3.superstep
		for(step=0;step<max;step++){
			for each aggr in aggregators{
				aggr.createInitialValue();
			}
			for each v in vertices{
				v.compute();
			}
		}
		//4.cleanup
		for each v in vertices{
			v.cleanup();
		}
		WorkerCoputer.cleanup();

1.10 了解大数据计算服务 DataHub 的相关概念和使用方法

	datahub概念：
		datahub是MaxCompute提供的流式数据处理服务，它提供流式数据的发布和订购的功能，让您可以轻松构建基于流式数据的分析和应用。datahub可以对各种移动设备，应用软件，网站服务，传感器等产生的大量流式数据进行持续不断的采集，存储和处理。用户可以编写应用程序或者使用流计算引擎来处理写入到datahub的流式数据比如实时web访问日志、应用日志、各种事件等，并产生各种实时的数据处理结果比如实时图表、报警信息、实时统计等。
		datahub基于阿里云自研的飞天平台，具有高可用，低延迟，高可扩展，高吞吐的特点。
		datahub与阿里云流计算引擎streamCompute无缝连接，用户可以轻松使用SQL进行流数据分析。
		datahub也提供流式数据归档的功能，支持流式数据归档进入MaxCompute

	datahub使用方法介绍
		streamCompute是阿里云提供的流计算引擎，提供使用类SQL的语言来进行流式计算。datahub和streamCompute无缝结合，可以作为stremaCompute的数据源和输出源。
		数据采集 datahub 事件触发 streamCompute 数据写入 datastore 数据展示/在线服务/离线计算

	流处理应用：
		用户可以编写应用订阅datahub中的数据，并进行实时的加工，把加工后的结果输出。用户可以把应用计算产生的结果输出到datahub中，并使用另外一个应用来处理上一个应用生成的流式数据，来构建数据处理流程的dag

	流式数据归档
		用户的流式数据可以归档到MaxCompute中，用户通过参加datahubConnector,指定相关配置，即可创建将datahub中流式数据定期归档的同步任务
		project	项目是datahub数据的基本组织单元，下面包含多个topic，值得注意的是，datahub的项目与maxCompute的项目空间是相互独立的。用户在maxCompute中创建的项目不能复用于datahub，需要独立创建。
		topic topic是datahub订阅和发布的最小单位，用户可以用topic来表示一类或者一种流数据
		topic lifecycle 表示一个topic中写入数据在系统中可以保持的最长时间，以天为单位
		shard shard表示对一个topic进行数据传输的兵法通道，每个shard会有对应的id，每个shard会有多种状态：opening-启动中，active-启动完成可服务。每个shard启动以后会占用一定的服务端资源，建议按需申请shard数量
		shard hash key range	每个shard都有的属性，包括开始和结束的key范围，写入数据的时候具有相同key的数据会落到同一个shard上，对一个shard的key范围是左闭右开
		shard merge shard合并，可以把相邻的key range相连接的两个shard
		record 用户数据和datahub端交互的基本单位
		recordtype topic的数据类型，目前支持tuple与blob两种类型，tuple类型的topic支持类似数据库的记录的数据，每条记录包含多个列。blob类型的topic仅支持写入一块二进制数据


1.11 熟悉大数据计算服务的安全和权限管理的概念和实际操作，包括用户、角色、授权（ACL&Policy）、项目空间保护、例外以及安全等级等

	01用户和角色	用户/角色管理：添加删除角色，Owner/Admin区别
	02授权方式		ACL授权、Policy授权
	03标签安全		打开/关闭 labelSecurity，显示授权优先级高
	04授权			package授权应用场景，优先级高于项目保护
	05项目空间保护	项目空间保护的应用场景，数据合规流出的三种方式

	大数据计算服务安全和权限概念介绍
		项目空间用户管理
		角色管理
			角色是一组访问权限的集合
				报表产生者
					读：每日交易明细
					读：每人交易汇总
					读：历史交易报表
				缺省角色
					访问项目空间所有对象
					进行用户与角色的管理
					对用户和角色进行授权
			相比Owner，Admin不能：
				1.将admin角色赋给别的用户
				2.不能设定项目空间的安全配置
				3.不能修改项目空间的鉴权模型
			角色的限制：
				1.admin角色的权限不能被修改
				2.没被使用的角色才可以被删除
			为什么移除用户的时候要保证该用户没有被赋予角色权限？

		大数据计算服务安全和权限概念操作
			授权：
				三个要素：主体、客体、操作
				两种方法：
					ACL 基于对象的授权
					Policy 基于策略的授权
											ACL 	Policy
			subject&object是否必须存在		是		否
			删除对象时，是否必须撤销相关授权 是		否
			是否支持白名单allow授权			是 		是
			是否支持黑名单deny授权			否 		是
			是否支持带限制条件的授权		否 		是
			是否支持通配符(*)				否 		是

		ACL授权
			基本语法：
				grant privileges on object to subject
				revoke privileges on object from subject
			云账号用户bob@aliyun.com是新加入到项目空间wonderLand的成员，他需要提交作业、创建数据表、查看项目空间已存在的对象
				use wonderland;
				security;
				add user aliyun$bob@aliyum.com;
				grant createinstance on project wanderland to user aliyum$bob@aliyun.com;...

		数据保护机制
			设置projectprotection规则：数据只能流入，不能流出
			set projectprotection=true
			设置后，各种数据导出操作将统统失效，因为它们都触犯了projectprotection规则。
			默认时，projectprotection不会被设置，需要手工开启

	大数据计算服务安全项目空间保护
		项目空间保护下的合规数据流出

			经过严格审查，发现bob将表customers导出到另一个项目空间secretGarden是符合规定的，现在已经设置了项目保护，alice需要考虑如何在不破坏目前保护机制的情况下，对bob的要求予以满足

				办法1：在设置项目保护projectprotection的同时，附加一个例外策略exception:
				set projectProtection=true with exception policyfile;

				方法2：将两个相关的项目空间设置为互信trustedProject，则数据的流向将不会被视为违规：
				add trustedproject=SecretGarden;

		查看权限
			支持查看指定用户的权限、查看指定角色的权限，以及查看指定对象的授权列表
			查看指定用户的权限
				show grants;
				show grants for username;

			查看指定角色的权限：
				describe role rolename;

			查看指定对象的授权列表：
				show acl for objectName on type objectType

		鉴权模型配置
			支持了多种正交的授权机制，用户可通过设置下列参数来定制项目空间的鉴权模型

			security.CheckPermissionUsingACL	激活/冬季ACL授权机制，默认为true
			security.CheckPermissionUsingPolicy	激活/冻结Policy授权机制，默认为true
			security.ObjectCreatorHasAccessPermission 允许/禁止对象创建者默认拥有访问权限，默认为true

		鉴权模型配置
			支持了多种正交的授权机制，用户可通过设置下列参数来定制项目空间的鉴权模型

			security.ObjectCreatorHasGrantPermission	允许/禁止对象创建者默认拥有授权权限，默认为true
			security.LableSecurity 开启/关闭labelSecurity安全策略，默认为false
			projectProtection 开启/关闭项目空间的数据保护机制，默认false

			查看鉴权模型：
			show securityconfiguration;
			强制访问控制策略(MAC)
			自主访问控制策略(DAC)

		LabelSecurity基本操作
			打开labelSecurity安全机制开关： set security.LabelSecurity=true|false;
			设置用户安全许可标签：	set label number to user username;
			设置数据敏感等级标签：	set label number to table tablename column_list;
			显示授权低级别用户访问高敏数据：	grant label number on table tablename column_list to user username with exp days;
			撤销显示授权：	revoke label on table tablename column_list from user username;

		LabelSecurity基本操作
			清理过期的显式授权：	clear expired grants;
			查看一个用户能访问哪些敏感数据集：	show label level grants for user username;
			查看一个敏感数据表能被哪些用户访问：	show label level grants on table tablename;
			用户对指定表上列级别的label授权：	show label level grants on table tablename for user username;
			包安装者对包中敏感资源许可访问级别：	allow project pjname to install package pkanme using label n;

			set label 1 to table t1;
			set label 2 to table t1(id,name);
			set label 3 to table t1;

			思考：经过三步操作后，字段id目前的敏感等级是？ 2

		跨项目空间资源分享
			跨项目空间的资源分享通过package的方式来进行操作
			创建者基本操作
			创建package: create package package_name;
			删除package: drop package package_name;
			添加想要分享的资源到package: add project_object to package package_name with privileges privileges;
			从package中去掉分享的对象： remove project_object from package package_name;
			允许其他项目空阿金使用此package： allow project project_name to install package package_name using label n;
			撤销项目空间对该package的使用许可： disallow project project_name to install package package_name;
			查看已安装的package: show package;
			查看package的详细信息： describe package package_name;

		使用者的基本操作
			卸载package： uninstall package project_name.package_name;
			查看已安装的package： show packages;
			查看package的详细信息：describe package package_name;

			说明：被安装的package是独立的对象类型，若要访问package里的资源(即其他项目空间分享的资源)，必须拥有对该package的read权限，如果请求者没有read权限，则需要向projectOwner或admin申请。projectOwner或admin可以通过ACL授权或Policy授权机制来完成

		数据保护机制：
			设置ProjectProtection规则：数据只能流入，不能流出
				set ProjectProtection=true
			设置后，上述的4种操作将统统失败，因为它们都触犯了ProjectProtection规则
			默认时，ProjectProtection不会被设置，需要手工开启
			规定：资源分享优先于数据保护
				trustedproject 、exception policy 、package

		授权语句结构：
			principle主体 访问策略中的权限被指派的对象
			action操作	主体对资源的访问方法
			resource资源 主体请求访问的对象
			access restriction访问限制 权限生效的限制条件
			effect效果	权限类型：允许或者拒绝

		访问策略语言结构：
			策略头部 可选 版本信息 Optional top-level elements
			策略主体 授权语句集合 Statement

		语言规范：
			Principal：
				指请求发送者的身份，目前仅支持阿里云账号、域账号和淘宝账号的身份表示
				对于云账号，可以支持ID或displayname表示，如："principal":aliyun$bob@aliyun.com
				如果授权绑定就是或者用户，则不需指定Principal，如果是项目空间或对象，则必须指定

			Resource：
				acs:<service-name>:<namespace>:<relative-id>
				relative_id的格式为：projects/<project_name>/<object_type>/<object_name>
			Actioin:
				<service-name>:<action-name>
			Condition keys:
				acs:<condition-key>
				具体服务效果的条件关键字的命令格式为：<service-name>:<condition-key>

			例子：
				{
				"Version":"1",
				"Statement":
				[{
					"Effect":"Allow",
					"Principal":"*",
					"Action":["odps:Read"],
					"Resource":"acs:odps:*:projects/SecretGarden/packages/WonderLand.pkg_wl"}]
				}

				user SecretGarden;
				Install package WonderLand.pkg_wl;
				put policy ./pkg_wl_toAll.JSON
				
二、数据工场 DataWorks 考试内容：
2.1熟悉 DataWorks 的基本功能模块，包括数据开发、数据管理、运维中心、组织管理以及项目管理等

	数据管理
		云数据查看
		云数据管理
		血缘关系分析
		类目管理
		两种方式建表
	角色隔离
		角色分类
		角色权限
	项目配置
		组织
		多环境隔离
		项目属性管配

	角色隔离
	1.组织管理员
		指组织的管理者，可新建计算引擎，新建项目空间，新建调度资源，添加组织成员，为组织成员赋予组织管理角色、配置数据类目等
	2.项目管理员
		指项目空间的管理者，可针对项目空间基本属性，数据源，当前项目空间计算引擎配置和项目成员进行管理，并为组织成员赋予项目管理员、开发、运维、部署、访客角色
	3.开发
		开发角色用户能够创建工作流，脚本文件、资源和UDF，新建表，同时可以创建发布包，但不能执行发布操作
	4.运维
		项目空间的运维人员，由项目管理员/项目所有者分配运维权限；拥有发布及线上运维的操作权限，无数据开发的操作权限
	5.部署
		部署角色与运维角色相似，但是其没有线上运维操作权限
	6.访客
		访客角色的用户只具备查看权限，而无权限进行编辑工作流和代码等

	组织：
		在云计算议题中，组织是指使用系统或计算资源的客户，即公司
	项目空间：
		项目空间是大数据开发平台最基本的组织对象，类似于传统数据库的database，阿里云大数据开发平台的项目空间，是进行多组织隔离和访问控制的主要边界，也是用户管理表table、资源resource、自定义函数UDF、节点node、工作流workflow、权限等的基本单元。在大数据开发平台中，一个项目空间对应绑定一个project
	作业：
		作业是由一个或多个节点(也称任务)，及其执行次序关系的工作流(workflow)组成，作业是一个静态概念，一个已定义的作业可以被反复执行
	工作流：
		工作流是一个DAG图(有向无环图)，其描述了作业中多个节点之间的逻辑(依赖关系)和规则(运行约束)

	构建数据仓库常见流程：
		数据产生--数据收集与存储--数据分析与处理--数据提取--数据展现与分享

	Datework的数据开发模块：
		数据源RDS/OTS--数据同步--大数据计算服务--按天加载--大数据开发平台

	数据集成-字段映射
		映射关系
		使用函数
		使用常量
		使用变量
		系统变量
			${bdp.system.bizdate}
			${bdp.system.cyctime}
		自定义变量
		自定义变量赋值
			[yyyy-mm-dd-1]
			[yyyy-mm--dd+1/24]
		使用变量导入到分区表

	数据集成-导入导出规则
		过滤条件
		清理规则
		速率配置
		容错
		主键冲突处理

	常见操作：
		1.查看、监控任务实例
		2.重跑任务（三种重跑方式）
		3.补数据（补不同个数周期数据）
		4.修改配置（部分属性可批量修改）

	组织管理模块

	项目管理模块				

2.2了解 DataWorks 的基本特点，包括角色隔离、环境隔离等

	角色隔离
	环境隔离
2.3可以使用项目管理和组织管理模块搭建环境
2.4熟练使用 DataWorks 的数据开发模块进行设计开发，包括建表、任务开发、资源上传、数据上传、新增函数等

	建表
	任务开发
	资源上传 
	新增函数
2.5熟练使用 DataWorks 的数据开发模块进行工作流任务和节点任务的开发设计，并且可以配置合适的依赖和周期性调度

	任务
		工作流任务
		节点任务
	调度
		周期性调度
		一次性任务
		调度粒度
		支持的任务类型
	依赖
		跨周期依赖
		跨项目依赖
		不同粒度依赖
	变量使用
		系统时间变量
		自定义变量 
	
	工作流任务开发设计
	
	节点任务的开发设计
		配置合适的依赖和周期性调度
	
	调度属性
		调度状态：是否勾选暂停，默认不勾选，任务暂停后，每天都会自动调度但是实例会直接返回失败状态，不会真正的运行任务逻辑
		调度生效日期：调度将在有效日期内生效并自动调度，反之，在有效期外的工作流将不会自动调度，也不能手动调度
		调度周期：设置工作流调度频次，支持天/周/月/分钟/小时
		【说明】：当创建一次性调度工作流任务时，工作流不具备调度属性配置选项；只有创建周期性调度工作流任务时，工作流任务才具有调度属性配置选项
		
		依赖属性
			自动推荐：系统自动扫描节点代码解析出来源和目标表从而推荐来源表是哪个任务产出，只有SQL类型节点任务和工作流任务有这个功能，同时也只能解析到SQL任务产出的表
			
			所属项目：当前组织内所有项目空间，可在下拉列表中进行选择(任务可以跨项目)
			
			上游任务：对应所属项目空间中的任务，用来设置当前任务的上有任务，非必填项。支持工作流名称模糊匹配查询
	
	跨周期依赖：
		配置节点/工作流任务的跨周期依赖，如，天调度任务今天需要咨询的数据依赖本任务昨天执行的数据，那么可以配置依赖任务昨天的周期，如此，昨天的实例必须先执行成功，今天的实例才可以调度起来，这种依赖主要是体现在任务调度实例的依赖
		不依赖上一调度周期，所有任务默认选择该选项，即不依赖任何任务的上周期实例
		
		自依赖，等待上一调度周期结束，才能继续运行。使用场景：任务A当前周期数据来源依赖与任务A上周期执行的结果；或者小时/分钟调度任务A不允许实例并行
		
		等待下游任务的上一周期结束，才能继续运行。依赖第一层子任务的上周期，这种使用场景不多，选择此项，后续该任务一旦被其他任务直接依赖则实例都依赖所有第一层子任务的上周期实例
		
		等待自定义任务的上一周期结束，才能继续运行
		
		常见场景：填任务A依赖一个数据是天任务B昨天产生的
		
	ODS(操作数据层)：
		同步：结构化数据增量或全量同步到ODPS;
		结构化：非结构化(日志)结构化处理并储存到odps;
		累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、数据清洗；
	DW(数据仓库层)：
		组合相关和相似数据：采用明细宽表，复用关联计算，减少数据扫描
		公共指标统一加工：基于公司规范体系构建命名规范、口径一致和算法统一的统计指标，为上层数据产品、应用和服务提供公共指标；建立逻辑汇总宽表
	DM(数据应用层)
		个性化指标加工：不公用性；复杂性(指数性、比值型、排名型指标)
		基于应用的数据组装：大宽表集市、横表转纵表、趋势指标串
2.6熟练使用数据管理模块进行数据管理，包括血缘分析、表的使用权限申请和授权等

	使用权限申请
	使用权限授权
2.7对于使用过程中出现的问题，能够识别、定位，对其中基本的问题能进行修复


三、数据集成考试内容
3.1了解数据集成的基本概念和工作流程

	设置切分
		目的、字段类型、数据源、满足条件
	DataX
		功能、数据源、目标类型、自动调度
	变量使用
		系统变量、自定义变量、使用变量导入到分区
	导入导出规则
		过滤条件、清理规则、速率配置、容错、主键冲突
	字段映射
		映射关系、使用函数、使用常量、使用变量
	问题排查
		权限问题、变量使用问题、脏数据处理等
3.2熟练使用数据集成进行多种场景下的数据同步，可以从不同的数据源同步数据到大数据计算服务

	数据同步的场景和同步操作配置
3.3了解数据集成中的 DataX 组件，包括概念、功能以及实际使用

	DataX的产品概念
	DataX的产品功能
	DataX的实际使用
	
	DataX使用json作为配置文件的格式
	
3.4了解并能正确使用数据集成中的字段映射、常量使用、变量使用、导入导出规则等
	
3.5了解数据集成的最佳实践，比如切分键配置等
3.6能够对使用过程中出现的常见问题进行排查，并能解决其中的基本问题

	对于数据集成中的脏数据能够进行正确的处理

四、应用和架构考试内容
4.1了解其他相关云产品的特点和应用场景，包括云数据库RDS、分布式关系型据库 DRDS、表格存储（TableStore）、分析型数据库（AnalyticDB）、对象存储 OSS 等

	表格存储
		大规模可扩展（单表数据量无限制）
		高吞吐低延时（百万级TPS、毫秒级延迟）
		自由表结构和宽行（单行包含的列数不限）
		不支持关联、计算等
	分析型数据库
		针对海量数据的灵活分析
		毫秒级多个大表关联计算
		标准SQL简单的使用方式
		不支持事务型、数据挖掘
	流计算
		实时、无界
		持续、高效
		流失、数据集成
		不适合复杂计算
	对象存储
		方便、灵活的使用方式
		强大、灵活的安全机制
		强大的数据处理能力
		非结构化数据
	云数据库
		稳定可靠、可弹性伸缩
		兼容多种关系型数据库
		备份回滚、性能监控及分析等功能
		只读实例、临时实例和灾备实例
	分布式关系型数据库
		基于RDS的分布式产品
		水平拆分、可平滑扩容
		支持大部分的标准SQL
		合理拆分支持大数据场景
		
4.2了解大数据计算服务、数据工场 DataWorks、数据集成、QuickBI及机器学习 PAI 等如何与其他相关产品配合使用
4.3能根据实际的应用场景，结合阿里云产品的特点设计合理的基础架构
